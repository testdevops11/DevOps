Docker compose and docker swarm : 

Compose Overview:

https://docs.docker.com/compose/

Install Docker Compose: (RHEL)

** Make sure docker is already installed if not:

$ sudo dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo

$ dnf install docker-ce --nobest -y

$ systemctl start docker

$ systemctl enable docker

https://docs.docker.com/compose/install/


############### Installation ##################


$ sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

 $ sudo chmod +x /usr/local/bin/docker-compose


 $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose


 $ docker-compose --version



Demo : webapp usign python and redis:

1. Create a directory for the project:

$ mkdir composetest
$ cd composetest

2. Create a file called app.py in your project directory and paste this in:

import time

import redis
from flask import Flask

app = Flask(__name__)
cache = redis.Redis(host='redis', port=6379)

def get_hit_count():
    retries = 5
    while True:
        try:
            return cache.incr('hits')
        except redis.exceptions.ConnectionError as exc:
            if retries == 0:
                raise exc
            retries -= 1
            time.sleep(0.5)

@app.route('/')
def hello():
    count = get_hit_count()
    return 'Hello World! I have been seen {} times.\n'.format(count)



 3.Create another file called requirements.txt in your project directory and paste this in:

flask
redis


Step 2: Create a Dockerfile

In your project directory, create a file named Dockerfile and paste the following:

# syntax=docker/dockerfile:1
FROM python:3.7-alpine
WORKDIR /code
ENV FLASK_APP=app.py
ENV FLASK_RUN_HOST=0.0.0.0
RUN apk add --no-cache gcc musl-dev linux-headers
COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt
EXPOSE 5000
COPY . .
CMD ["flask", "run"]


This tells Docker to:

Build an image starting with the Python 3.7 image.
Set the working directory to /code.
Set environment variables used by the flask command.
Install gcc and other dependencies
Copy requirements.txt and install the Python dependencies.
Add metadata to the image to describe that the container is listening on port 5000
Copy the current directory . in the project to the workdir . in the image.
Set the default command for the container to flask run.



Step 3: Define services in a Compose file

Create a file called docker-compose.yml in your project directory and paste the following:

version: "3.9"
services:
  web:
    build: .
    ports:
      - "5000:5000"
  redis:
    image: "redis:alpine"


 Step 4: Build and run your app with Composeüîó
From your project directory, start up your application by running docker-compose up.

$ docker-compose up

Creating network "composetest_default" with the default driver
Creating composetest_web_1 ...
Creating composetest_redis_1 ...
Creating composetest_web_1
Creating composetest_redis_1 ... done
Attaching to composetest_web_1, composetest_redis_1
web_1    |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)
redis_1  | 1:C 17 Aug 22:11:10.480 #



Open the port 5000 in secutity group and check the output.

Step 5: Edit the Compose file to add a bind mount
Edit docker-compose.yml in your project directory to add a bind mount for the web service:

version: "3.9"
services:
  web:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - .:/code
    environment:
      FLASK_ENV: development
  redis:
    image: "redis:alpine"


Step 6: Re-build and run the app with Compose
From your project directory, type docker-compose up to build the app with the updated Compose file, and run it.

$ docker-compose up



Step 7: Update the application
Because the application code is now mounted into the container using a volume, you can make changes to its code and see the changes instantly, without having to rebuild the image.

Change the greeting in app.py and save it. For example, change the Hello World! message to Hello from Docker!:

return 'Hello from Docker! I have been seen {} times.\n'.format(count)


Step 8: Experiment with some other commands
If you want to run your services in the background, you can pass the -d flag (for ‚Äúdetached‚Äù mode) to docker-compose up and use docker-compose ps to see what is currently running:

$ docker-compose up -d




##################### Docker Swarm: 

Overview:
https://docs.docker.com/engine/swarm/key-concepts/

What is a swarm?
The cluster management and orchestration features embedded in the Docker Engine are built using swarmkit. Swarmkit is a separate project which implements Docker‚Äôs orchestration layer and is used directly within Docker.

A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services). A given Docker host can be a manager, a worker, or perform both roles. When you create a service, you define its optimal state (number of replicas, network and storage resources available to it, ports the service exposes to the outside world, and more). Docker works to maintain that desired state. For instance, if a worker node becomes unavailable, Docker schedules that node‚Äôs tasks on other nodes. A task is a running container which is part of a swarm service and managed by a swarm manager, as opposed to a standalone container.

Nodes
A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.

Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.

Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.


Services and tasks
A service is the definition of the tasks to execute on the manager or worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.

When you create a service, you specify which container image to use and which commands to execute inside running containers.

In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.

For global services, the swarm runs one task for the service on every available node in the cluster.

A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.

Load balancing
The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.

External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.

Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.




Key Concepts:
https://docs.docker.com/engine/swarm/key-concepts/


How swarm works:

https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/



########################### Docker swarm installation:###########################


Prequisites:

1. Docker must be install on all the nodes which will become part of cluster.

2. The master nodes must have fixed IP - we will require elastic IP on aws 

3. Open protocols and ports between the hosts
The following ports must be available. On some systems, these ports are open by default.

TCP port 2377 for cluster management communications
TCP and UDP port 7946 for communication among nodes
UDP port 4789 for overlay network traffic


Either create new-security group and attach it to all the instances or add rules individually.

4. Login into the master node.

5. docker swarm init --advertise-addr 18.216.127.49 



To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-26jnftexpan9lyjn5bt7ok5x6ux1hclaowlgi2en1vcires138-chmjdst7wdgklhicuif9lq054 18.216.127.49:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

Check if the node is initiated as a leader:

$ docker node ls
ID                            HOSTNAME                                      STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
nfnsmm522sze7u0wxbwurvj1c *   ip-172-31-37-235.us-east-2.compute.internal   Ready     Active         Leader           20.10.12



6. Login into the slave nodes 

Run the swarm joing:  (both the nodes)

# docker swarm join --token SWMTKN-1-26jnftexpan9lyjn5bt7ok5x6ux1hclaowlgi2en1vcires138-chmjdst7wdgklhicuif9lq054 18.216.127.49:2377
This node joined a swarm as a worker.


7. check the master node docker node ls output:


# docker node ls
ID                            HOSTNAME                                      STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
nfnsmm522sze7u0wxbwurvj1c *   ip-172-31-37-235.us-east-2.compute.internal   Ready     Active         Leader           20.10.12
5e3us9t7001idhq4a5mebx4xw     ip-172-31-46-241.us-east-2.compute.internal   Ready     Active                          20.10.12
kwfwn1ucmppetffxxhgd6hzg3     master-node                                   Ready     Active                          20.10.12

Deploy a service to the swarm:


[root@ip-172-31-37-235 ~]# docker service create --replicas 1 --name helloworld alpine ping docker.com
e1j6qpjbbqvrnj7gfymj883b0
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

[root@ip-172-31-37-235 ~]# docker service ls
ID             NAME         MODE         REPLICAS   IMAGE           PORTS
e1j6qpjbbqvr   helloworld   replicated   1/1        alpine:latest

[root@ip-172-31-37-235 ~]# docker service ps helloworld
ID             NAME           IMAGE           NODE          DESIRED STATE   CURRENT STATE                ERROR     PORTS
a6rqhckmr75a   helloworld.1   alpine:latest   master-node   Running         Running about a minute ago




[root@ip-172-31-37-235 ~]# docker service ls
ID             NAME         MODE         REPLICAS   IMAGE           PORTS
e1j6qpjbbqvr   helloworld   replicated   1/1        alpine:latest

[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]# docker service scale helloworld=3
helloworld scaled to 3
overall progress: 3 out of 3 tasks
1/3: running   [==================================================>]
2/3: running   [==================================================>]
3/3: running   [==================================================>]
verify: Service converged
[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]# docker service ls
ID             NAME         MODE         REPLICAS   IMAGE           PORTS
e1j6qpjbbqvr   helloworld   replicated   3/3        alpine:latest
[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]#
[root@ip-172-31-37-235 ~]# docker service ps helloworld
ID             NAME           IMAGE           NODE                                          DESIRED STATE   CURRENT STATE            ERROR     PORTS
a6rqhckmr75a   helloworld.1   alpine:latest   master-node                                   Running         Running 22 minutes ago
k6a355mxssta   helloworld.2   alpine:latest   ip-172-31-37-235.us-east-2.compute.internal   Running         Running 26 seconds ago
i5ygftnzq7q8   helloworld.3   alpine:latest   ip-172-31-46-241.us-east-2.compute.internal   Running         Running 27 seconds ago
[root@ip-172-31-37-235 ~]#









